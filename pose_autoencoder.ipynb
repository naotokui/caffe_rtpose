{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "# constants\n",
    "\n",
    "pnames = [\"Nose\",#0\n",
    "          \"Neck\",#1 \n",
    "          \"RShoulder\",#2 \n",
    "          \"RElbow\", #3\n",
    "          \"RWrist\", #4\n",
    "          \"LShoulder\",#5\n",
    "          \"LElbow\",#6\n",
    "          \"LWrist\",#7\n",
    "          \"RHip\",#8\n",
    "          \"RKnee\",#9\n",
    "          \"RAnkle\",#10\n",
    "          \"LHip\",#11\n",
    "          \"LKnee\",#12\n",
    "          \"LAnkle\",#13\n",
    "          \"REye\",#14\n",
    "          \"LEye\",#15\n",
    "          \"REar\",#16\n",
    "          \"LEar\",#17\n",
    "          \"Bkg\"]#18\n",
    "\n",
    "# find connection in the specified sequence, center 29 is in the position 15\n",
    "limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from scipy.misc import imresize\n",
    "\n",
    "# If your GPU supports CUDA and Caffe was built with CUDA support,\n",
    "# uncomment the following to run Caffe operations on the GPU.\n",
    "# caffe.set_mode_gpu()\n",
    "# caffe.set_device(0) # select GPU device if multiple devices exist\n",
    "\n",
    "def showarray(a, fmt='jpeg', resize=True, height=360, width=360):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    a = imresize(a, [height, width], 'bilinear')\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "\n",
    "NUM_TOTAL_JOINTS = 18\n",
    "\n",
    "class Body:\n",
    "\n",
    "    \"\"\" representing one body with joints info \"\"\"\n",
    "    def __init__(self, joints_raw, parts_thresh = 0.0, conf_thresh=0.0, min_size_thresh=0):\n",
    "        \"\"\" initializing with joints array \n",
    "        \n",
    "        parts_thresh: (visible joints/total joints). \n",
    "                    body with the number of valid joints under this threshold will be regarded as invalid\n",
    "        conf_thresh: joints with confidence under this threshold will be ignored\n",
    "        min_size_thresh: max(body width, body height) should be bigger than this threshold\n",
    "        \"\"\"\n",
    "        assert len(joints_raw) == 54, \"Invlid joint array\"\n",
    "        self.joints_raw = joints_raw\n",
    "        self.joints = []\n",
    "        self.parts_thresh = parts_thresh\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.num_joints_valid = 0\n",
    "    \n",
    "        # create joints array\n",
    "        for i in range(0,len(joints_raw), 3):\n",
    "            self.joints.append(joints_raw[i:i+3])\n",
    "            if joints_raw[i+2] > conf_thresh:\n",
    "                self.num_joints_valid += 1\n",
    "        self.joints_array = self.joints # keep this for convenience\n",
    "        self.joints = np.array(self.joints)\n",
    "        \n",
    "        if self.num_joints_valid / float(NUM_TOTAL_JOINTS) < parts_thresh:\n",
    "            self.is_valid = False\n",
    "            return\n",
    "        \n",
    "        # get size\n",
    "        _joints = []\n",
    "        for _j in self.joints_array:\n",
    "            if _j[2] > self.conf_thresh:\n",
    "                _joints.append(_j[:2]) # ignore invalid joint\n",
    "        _joints = np.array(_joints)\n",
    "        min_x = np.min(_joints[:,0])\n",
    "        max_x = np.max(_joints[:,0]) \n",
    "        min_y = np.min(_joints[:,1])\n",
    "        max_y = np.max(_joints[:,1]) \n",
    "        size  = max(max_x - min_x, max_y - min_y)\n",
    "        \n",
    "        # get normalized joints\n",
    "        self.norm_joints = self.get_normalized_joints_matrix()\n",
    "\n",
    "        # enough visible joints and bigger enough\n",
    "        self.is_valid = (size > min_size_thresh and self.norm_joints is not None)         \n",
    "        \n",
    "    def is_valid_joint(self, index1, index2):\n",
    "        \"\"\" check if a joint between index1 to index2 is valid \n",
    "        \"\"\"\n",
    "        return (self.joints_array[index1][2] > self.conf_thresh and self.joints_array[index2][2] > self.conf_thresh)\n",
    "        \n",
    "    def get_cropped_image_around_body(self, canvas, margin_coef = 1.25):\n",
    "        \"\"\" crop image around find body\n",
    "        \"\"\"\n",
    "        assert self.is_valid, \"this body has not enough valid joints!\"\n",
    "        \n",
    "        _joints = []\n",
    "        for _j in self.joints_array:\n",
    "            if _j[2] > self.conf_thresh:\n",
    "                _joints.append(_j[:2])\n",
    "        \n",
    "        _joints = np.array(_joints)\n",
    "        min_x = np.min(_joints[:,0])\n",
    "        max_x = np.max(_joints[:,0]) \n",
    "        min_y = np.min(_joints[:,1])\n",
    "        max_y = np.max(_joints[:,1]) \n",
    "        center = [0.5*(max_x+min_x),0.5*(max_y+min_y)]\n",
    "        size  = max(max_x - min_x, max_y - min_y) * margin_coef \n",
    "          \n",
    "        x1 = int(min(max(0, center[0] - size*0.5), canvas.shape[1]))\n",
    "        x2 = int(min(max(0, center[0] + size*0.5), canvas.shape[1]))\n",
    "        y1 = int(min(max(0, center[1] - size*0.5), canvas.shape[0]))\n",
    "        y2 = int(min(max(0, center[1] + size*0.5), canvas.shape[0]))\n",
    "  \n",
    "        cropped = canvas[y1:y2, x1:x2]\n",
    "        return cropped\n",
    "    \n",
    "    def get_normalized_joints_matrix(self):\n",
    "        \"\"\"\n",
    "        normalized for learning.\n",
    "        center = between left \n",
    "        \"\"\"\n",
    "        \n",
    "        joints = np.copy(self.joints)\n",
    "        \n",
    "        # TODO: how to handle invalid joint???\n",
    "        # at least you should have valid hips!\n",
    "        if np.min(joints[(pnames.index(\"RHip\"), pnames.index(\"LHip\")),2]) < self.conf_thresh:\n",
    "            return None;\n",
    "        \n",
    "        # body center is between hips\n",
    "        body_center = np.mean(joints[(pnames.index(\"RHip\"), pnames.index(\"LHip\")),:2], axis=0)         \n",
    "        \n",
    "        # assume that the less confident parts are at the center of body\n",
    "        joints[joints[:,2] < self.conf_thresh, :2] = body_center\n",
    "        dists = joints[:, :2] - body_center        \n",
    "        max_dist = np.max(np.linalg.norm(dists, axis=1))\n",
    "        norm_dists = dists/max_dist\n",
    "        return norm_dists\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91948/91948 [05:39<00:00, 270.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16318, 36)\n",
      "# of found bodies 16318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "filepaths = glob.glob(\"./video/olympics_frames2/*.json\")\n",
    "random.shuffle(filepaths)\n",
    "\n",
    "body_mat = None\n",
    "body_info = []\n",
    "\n",
    "for jsonpath in tqdm(filepaths):\n",
    "    with open(jsonpath) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        bodies = d['bodies']\n",
    "        for _b in bodies:\n",
    "            body = Body(_b['joints'], parts_thresh=0.90, conf_thresh=0.40, min_size_thresh=180)\n",
    "\n",
    "            if body.is_valid:\n",
    "                imagepath = jsonpath[:-5]+\".bmp\"\n",
    "                \n",
    "                if body_mat is None:\n",
    "                    body_mat = body.norm_joints.reshape(1, 36)\n",
    "                else:\n",
    "                    body_mat = np.vstack((body_mat, body.norm_joints.reshape(1,36)))\n",
    "                body_info.append({\"image\": imagepath, \"body\": body})\n",
    "\n",
    "body_mat = np.array(body_mat)\n",
    "print body_mat.shape\n",
    "print \"# of found bodies\", len(body_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16318, 36)\n",
      "# of found bodies 16318\n"
     ]
    }
   ],
   "source": [
    "body_mat = np.array(body_mat)\n",
    "np.savez(\"body_mat.npz\", body_mat = body_mat, body_info = body_info)\n",
    "print body_mat.shape\n",
    "print \"# of found bodies\", len(body_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16318, 36)\n",
      "16318\n"
     ]
    }
   ],
   "source": [
    "body_mat = np.load(\"body_mat.npz\")[\"body_mat\"]\n",
    "body_info = np.load(\"body_mat.npz\")[\"body_info\"]\n",
    "print body_mat.shape\n",
    "print len(body_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder\n",
    "# encoding: utf-8\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# encodeded representation\n",
    "input_size = body_mat.shape[1]\n",
    "encoding_dim = 12\n",
    "\n",
    "input_vector = Input(shape=(input_size,)) # 784 = 28 x 28\n",
    "x1 = Dense(encoding_dim * 2, activation='relu')(input_vector)\n",
    "encoded = Dense(encoding_dim)(x1)\n",
    "x2 = Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "decoded = Dense(input_size)(x2)\n",
    "\n",
    "# autoencoder\n",
    "autoencoder = Model(input=input_vector, output=decoded)\n",
    "\n",
    "# encoder only model\n",
    "encoder = Model(input=input_vector, output=encoded)\n",
    "\n",
    "# decoder\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "x2_layer = autoencoder.layers[-2]\n",
    "decoder_layer = autoencoder.layers[-1] # 最後のlayer\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(x2_layer(encoded_input)))\n",
    "\n",
    "# compile\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14686 samples, validate on 1632 samples\n",
      "Epoch 1/150\n",
      "0s - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 2/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 3/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 4/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 5/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 6/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 7/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 8/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 9/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 10/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 11/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 12/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 13/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 14/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 15/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 16/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 17/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 18/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 19/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 20/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 21/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 22/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 23/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 24/150\n",
      "0s - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 25/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0031\n",
      "Epoch 26/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 27/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 28/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 29/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 30/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 31/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 32/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 33/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 34/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 35/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 36/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 37/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 38/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 39/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 40/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 41/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 42/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 43/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 44/150\n",
      "0s - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 45/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 46/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 47/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 48/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0028\n",
      "Epoch 49/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 50/150\n",
      "0s - loss: 0.0029 - val_loss: 0.0029\n",
      "Epoch 51/150\n",
      "0s - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 52/150\n",
      "0s - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 53/150\n",
      "0s - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 54/150\n",
      "0s - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 55/150\n",
      "0s - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 56/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 57/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 58/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 59/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 60/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 61/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 62/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 63/150\n",
      "0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 64/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 65/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 66/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 67/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 68/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 69/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 70/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 71/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 72/150\n",
      "0s - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 73/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 74/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 75/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 76/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 77/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 78/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 79/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 80/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 81/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 82/150\n",
      "0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 83/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 84/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 85/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 86/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 87/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 88/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 89/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 90/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 91/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 92/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 93/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 94/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 95/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 96/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 97/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 98/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 99/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 100/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 101/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 102/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 103/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 104/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 105/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 106/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 107/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 108/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 109/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 110/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 111/150\n",
      "0s - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 112/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 113/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 114/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 115/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 116/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 117/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 118/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 119/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 120/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 121/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 122/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 123/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 124/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 125/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 126/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 127/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 128/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 129/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 130/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 131/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 132/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 133/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 134/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 135/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 136/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 137/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 138/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 139/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 140/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 141/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 142/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 143/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 144/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 145/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 146/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 147/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 148/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 149/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 150/150\n",
      "0s - loss: 0.0023 - val_loss: 0.0023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98825d0450>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input = outputなのでx_trainが並んでる\n",
    "autoencoder.fit(body_mat, body_mat, nb_epoch=150, batch_size=256, shuffle=True, validation_split=0.1,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "autoencoder.save(\"pose_autoencoder.h5\")\n",
    "encoder.save(\"pose_autoencoder_encoder.h5\")\n",
    "decoder.save(\"pose_autoencoder_decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nao/anaconda2/envs/default/lib/python2.7/site-packages/keras/models.py:150: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "autoencoder = load_model(\"pose_autoencoder.h5\")\n",
    "encoder = load_model(\"pose_autoencoder_encoder.h5\")\n",
    "decoder = load_model(\"pose_autoencoder_decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 全部ロード仕様とおもすぎるので最初の　num_embeddingに絞る\n",
    "num_embedding = 2000\n",
    "\n",
    "encoded_mat = encoder.predict(body_mat[:num_embedding,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=/tmp/tf_logs/20170223-155659/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"/tmp/tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "cmd = \"tensorboard --logdir=\" + logdir\n",
    "print cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:14<00:00, 139.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a variable \n",
    "    # 表現したいベクトルは１次元であるひつようあり！\n",
    "    embedding_var = tf.Variable(encoded_mat, trainable=False, name=\"encoded_vector\")\n",
    "    sess.run(embedding_var.initializer)\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    summary_writer = tf.summary.FileWriter(logdir)\n",
    "    \n",
    "    # file id\n",
    "    metadata_path = os.path.join(logdir, 'metadata.tsv')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for name in range(num_embedding):\n",
    "            f.write('%s\\n' % name)\n",
    "    embedding.metadata_path = metadata_path\n",
    "    \n",
    "    # image\n",
    "    images = []\n",
    "    image_dim = 100\n",
    "    for i in tqdm(range(num_embedding)):\n",
    "        imagepath = body_info[i]['image']\n",
    "        foundbody = body_info[i]['body']\n",
    "        canvas = cv.imread(imagepath) # B,G,R order\n",
    "        cropped = foundbody.get_cropped_image_around_body(canvas)\n",
    "        cropped = cropped[:,:,[2,1,0]]\n",
    "        resized = tf.image.resize_images(cropped, [image_dim, image_dim])\n",
    "#        resized = tf.image.rgb_to_grayscale(resized)\n",
    "        images.append(tf.cast(resized, tf.uint8))\n",
    "        \n",
    "    image_path = os.path.join(logdir, 'sprite.jpg')\n",
    "    size = int(math.sqrt(len(images))) + 1\n",
    "    while len(images) < size * size:\n",
    "        images.append(np.zeros((image_dim, image_dim, 3), dtype=np.uint8))        \n",
    "    rows = []\n",
    "    for i in range(size):\n",
    "        rows.append(tf.concat(images[i*size:(i+1)*size],1))\n",
    " \n",
    "    jpeg = tf.image.encode_jpeg(tf.cast(tf.concat(rows, 0), tf.uint8))\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(sess.run(jpeg))\n",
    "    embedding.sprite.image_path = image_path\n",
    "    embedding.sprite.single_image_dim.extend([image_dim, image_dim])\n",
    "        \n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    sess.run([embedding_var])\n",
    "    saver = tf.train.Saver([embedding_var])\n",
    "    saver.save(sess, os.path.join(logdir, 'model.ckpt'))\n",
    "    print \"saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=/tmp/tf_logs/20170223-161629/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = \"/tmp/tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "cmd = \"tensorboard --logdir=\" + logdir\n",
    "print cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:14<00:00, 140.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "body_mat_short = body_mat[:num_embedding,:]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a variable \n",
    "    # 表現したいベクトルは１次元であるひつようあり！\n",
    "    embedding_var = tf.Variable(body_mat_short, trainable=False, name=\"encoded_vector\")\n",
    "    sess.run(embedding_var.initializer)\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    summary_writer = tf.summary.FileWriter(logdir)\n",
    "    \n",
    "    # file id\n",
    "    metadata_path = os.path.join(logdir, 'metadata.tsv')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        for name in range(num_embedding):\n",
    "            f.write('%s\\n' % name)\n",
    "    embedding.metadata_path = metadata_path\n",
    "    \n",
    "    # image\n",
    "    images = []\n",
    "    image_dim = 100\n",
    "    for i in tqdm(range(num_embedding)):\n",
    "        imagepath = body_info[i]['image']\n",
    "        foundbody = body_info[i]['body']\n",
    "        canvas = cv.imread(imagepath) # B,G,R order\n",
    "        cropped = foundbody.get_cropped_image_around_body(canvas)\n",
    "        cropped = cropped[:,:,[2,1,0]]\n",
    "        resized = tf.image.resize_images(cropped, [image_dim, image_dim])\n",
    "#        resized = tf.image.rgb_to_grayscale(resized)\n",
    "        images.append(tf.cast(resized, tf.uint8))\n",
    "        \n",
    "    image_path = os.path.join(logdir, 'sprite.jpg')\n",
    "    size = int(math.sqrt(len(images))) + 1\n",
    "    while len(images) < size * size:\n",
    "        images.append(np.zeros((image_dim, image_dim, 3), dtype=np.uint8))        \n",
    "    rows = []\n",
    "    for i in range(size):\n",
    "        rows.append(tf.concat(images[i*size:(i+1)*size],1))\n",
    " \n",
    "    jpeg = tf.image.encode_jpeg(tf.cast(tf.concat(rows, 0), tf.uint8))\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(sess.run(jpeg))\n",
    "    embedding.sprite.image_path = image_path\n",
    "    embedding.sprite.single_image_dim.extend([image_dim, image_dim])\n",
    "        \n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    sess.run([embedding_var])\n",
    "    saver = tf.train.Saver([embedding_var])\n",
    "    saver.save(sess, os.path.join(logdir, 'model.ckpt'))\n",
    "    print \"saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.         -0.09673796 -0.55154003  0.01077628 -0.56159826\n",
      "  0.10831514 -0.27013608  0.          0.         -0.23649818 -0.55150689\n",
      " -0.28001756 -0.25933218 -0.43060929 -0.20487082  0.09716879  0.\n",
      "  0.07531244  0.46437423  0.19430996  0.82102434 -0.09716879  0.         -0.4524546\n",
      "  0.3030504  -0.67829249  0.73479201  0.          0.          0.          0.\n",
      "  0.          0.         -0.29073308 -0.73476992]\n",
      "[[-0.0368489  -0.06802221 -0.03029673 -0.48934975  0.1107394  -0.48543811\n",
      "   0.09450564 -0.19057766 -0.09538137 -0.08128609 -0.19582306 -0.46150014\n",
      "  -0.38449973 -0.21244901 -0.45778859 -0.19601478  0.12433363 -0.00592352\n",
      "   0.17398603  0.41582155  0.302026    0.88152504 -0.11843937  0.00127022\n",
      "  -0.4461413   0.39817518 -0.49515843  0.72199726 -0.0525818   0.05771903\n",
      "  -0.04056739 -0.08941069 -0.00617    -0.02536397 -0.11257882 -0.67260206]]\n"
     ]
    }
   ],
   "source": [
    "print body_mat[0]\n",
    "encoded = encoder.predict(body_mat[0].reshape(1,36), 1)\n",
    "decoded = decoder.predict(encoded, 1)\n",
    "print decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
